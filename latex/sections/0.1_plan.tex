\section{Planning}
Many ML projects are doomed from the start due to a misalignment between product
metrics and model metrics. More projects fail by producing good models that
aren't helpful for a product rather than due to modeling difficulties.


% ML Eng, 2020. Burkov
\subsection{Prioritization of Machine Learning Projects}
The key considerations in the prioritization of a machine learning project,
are impact and cost.


\subsection*{Impact of Machine Learning}
The impact of using machine learning in a broader engineering project is high
when:
\begin{enumerate}
    \item machine learning can replace a complex part in your engineering
    project, or
    \item there's a great benefit in getting inexpensive (but probably
    imperfect) predictions.
\end{enumerate}

Inexpensive and imperfect predictions can be valuable, for example, in a
system that dispatches a large number of requests. Let's say many such
requests are “easy” and can be solved quickly using some existing automation.
The remaining requests are considered “difficult” and must be addressed manually.



\subsection*{Cost of Machine Learning}
Three factors highly influence the cost of a machine learning project:
\begin{itemize}
    \item the difficulty of the problem,
    \item the cost of data, and
    \item the need for accuracy
    \footnote{
        The need for high accuracy can translate into the requirement of
        getting more data or training a more complex model, such as a unique
        architecture of a deep neural network or a nontrivial ensembling
        architecture.
    }.
\end{itemize}
Getting the right data in the right amount can be very costly, especially if
manual labeling is involved.

When you think about the problem's difficulty, the primary considerations are:
\begin{itemize}
    \item whether an implemented algorithm or a software library capable of
    solving the problem is available (if yes, the problem is greatly simplified),

    \item whether significant computation power is needed to build the model or
    to run it in the production environment.
\end{itemize}

The second driver of the cost is data. The following considerations have to be made:
\begin{itemize}
    \item can data be generated automatically (if yes, the problem is greatly simplified),
    \item what is the cost of manual annotation of the data (i.e., assigning labels to unlabeled
    examples),
    \item how many examples are needed (usually, that cannot be known in advance, but can be
    estimated from known published results or the organization's own experience).
\end{itemize}

Finally, one of the most influential cost factors is the desired accuracy of the model. The
machine learning project's cost grows superlinearly with the accuracy requirement
Low accuracy can also be a source of significant loss when the model is
deployed in the production environment. The considerations to make:
\begin{itemize}
    \item how costly is each wrong prediction, and
    \item what is the lowest accuracy level below which the model becomes impractical.
\end{itemize}





% ML Eng, 2020. Burkov
\subsection{Estimating Complexity of a Machine Learning Project}
There is no standard complexity estimation method for a machine learning project, other than
by comparison with other projects executed by the organization or reported in the literature.



\subsection*{The Unknowns}
There are several major unknowns that are almost impossible to guess with confidence unless
you worked on a similar project in the past or read about such a project. The unknowns are:
\begin{itemize}
    \item whether the required quality is attainable in practice,
    \item how much data you will need to reach the required quality,
    \item what features and how many features are necessary so that the model can learn and
    generalize sufficiently,
    \item how large the model should be (especially relevant for neural networks and ensemble
    architectures), and
    \item how long will it take to train one model (in other words, how much time is needed to
    run one experiment) and how many experiments will be required to reach the desired
    level of performance.
\end{itemize}

\subsection*{Nonlinear Progress}
The progress of a machine learning project is nonlinear.

The prediction error usually decreases fast in the beginning, but then the progress gradually
slows down. Sometimes you see no  progress and decide to add additional features that could
potentially depend on external databases or knowledge bases. While you are working on a new 
feature or labeling more data (or outsourcing this task), no progress in model performance is
happening.

Because of this nonlinearity of progress, you should make sure that the product owner (or
the client) understands the constraints and risks. Carefully log every activity and track the
time it took. This will help not only in reporting, but also in the estimation of the complexity
of similar projects in the future.






\subsection{Measuring Success}
We have three potential approaches for increasing the complexity of ML models
\footnote{
    It is important to also realize that even features that could benefit from
    ML can often simply use a heuristic for their first version. Once the
    heuristic is being used, you may even realize that you do not need ML at all.
}:
\begin{enumerate}
    \item[1.] Baseline, design heuristics based on domain knowledge.
    \item[2.] Simple Model. 
    \item[3.] Complex Model.
\end{enumerate} 
All of these approaches are different and may evolve as we learn more from
prototypes along the way, but when working on ML, you should define a common
set of metrics to compare the success of modeling pipelines.

To that end, we will cover four categories of performance that have a
large impact on the usefulness of any ML product: business metrics,
model metrics, freshness, and speed. Clearly defining these metrics will
allow us to accurately measure the performance of each iteration.

\begin{itemize}
    \item \textbf{Business Performance}
    
    \noindent
    Once the product or feature goal is clear, a metric should be defined
    to judge its success. This metric should be separate from any model
    metrics and only be a reflection of the product's success
    \footnote{
        Product metrics may be as simple as the number of users a feature
        attracts or more nuanced such as the click-through rate (CTR) of
        the recommendations we provide.
    }.

    Product metrics are ultimately the only ones that matter, as they
    represent the goals of your product or feature. All other metrics
    should be used as tools to improve product metrics
    \footnote{
        Product metrics, however, do not need to be unique. While most
        projects tend to focus on improving one product metric, their
        impact is often measured in terms of multiple metrics,
        including guardrail metrics, metrics that shouldn't decline
        below a given point.
    }. 


    \item \textbf{Model Performance}
    
    \noindent
    The ultimate product metric that determines the success of a model
    is the proportion of visitors who use the output of a model out of
    all the visitors who could benefit from it.

    When a product is still being built and not deployed yet, it is not
    possible to measure usage metrics. To still measure progress, it is
    important to define a separate success metric called an offline
    metric or a model metric.

    Small changes to the interaction between the model and product can
    make it possible to use a more straightforward modeling approach
    and deliver results more reliably
    \footnote{e.g. Changing an interface so that a model's results can
    be omitted if they are below aconfidence threshold; Presenting a
    few other predictions or heuristics in addition to a model's top
    prediction; Communicating to users that a model is still in an
    experimental phase and giving them opportunities to provide feedback
    }.


    \item \textbf{Freshness and Distribution Shift}
    
    \noindent
    Even if a model is trained on an adequate dataset, many problems have
    a distribution of data that changes as time goes on. When the distribution
    of the data shifts, the model often needs to change as well in order to
    maintain the same level of performance. In general, a model can perform
    well on data it hasn't seen before as long as it is similar enough to
    the data it was exposed to during training.

    Not all problems have the same freshness requirements.

    Depending on your business problem, you should consider how hard it will
    be to keep models fresh. How often will you need to retrain models, and
    how much will it cost you each time we do so?


    \item \textbf{Speed}
    
    \noindent
    Ideally, a model should deliver a prediction quickly. This allows users
    to interact with it more easily and makes it easier to serve a model to
    many concurrent users. So how fast does a model need to be?


\end{itemize}



\subsection{Estimate Scope and Challenges}
In ML, success generally requires understanding the context of the task well,
acquiring a good dataset, and building an appropriate model. Let's elaborate.

\begin{itemize}
    \item \textbf{Leverage Domain Expertise}
    
    \noindent
    The simplest model we can start with is a heuristic: a good rule of thumb based on
    knowledge of the problem and the data. The best way to devise heuristics is to see
    what experts are currently doing. Most practical applications are not entirely novel.
    How do people currently solve the problem you are trying to solve?

    The second best way to devise heuristics is to look at your data. Based on your dataset,
    how would you solve this task if you were doing it manually?

    To identify good heuristics, I recommend either learning from experts in the field or
    getting familiar with the data.


    \item \textbf{Stand on the Shoulders of Giants}
    
    \noindent

    Have people solved similar problems? If so, the best way to get started is to understand
    and reproduce existing results. Look for public implementations either with
    similar models or similar datasets, or both.

\end{itemize}
Leveraging existing open code and datasets can help make
implementation faster. In the worst case, if none of the existing models performs well
on an open dataset, you now at least know that this project will require significant
modeling and/or data collection work.

If you have found an existing model that solves a similar task and managed to train it
on the dataset it was originally trained on, all that is left is to adapt it to your domain.
To do so, I recommend going through the following successive steps:
\begin{enumerate}
    \item[1.] Find a similar open source model, ideally paired with a dataset it was trained on,
    and attempt to reproduce the training results yourself.

    \item[2.] Once you have reproduced the results, find a dataset that is closer to your use
    case, and attempt to train the previous model on that dataset.

    \item[3.] Once you have integrated the dataset to the training code, it is time to judge how
    your model is doing using the metrics you defined and start iterating.
\end{enumerate}



\subsection{To Make Regular Progress: Start Simple}
It is worth repeating that much of the challenge in ML is similar to one
of the biggest challenges in software—resisting the urge to build pieces
that are not needed yet.

Many ML projects fail because they rely on an initial data acquisition and
model building plan and do not regularly evaluate and update this plan.

Because of the stochastic nature of ML, it is extremely hard to predict how
far a given dataset or model will get us.

For that reason, it is vital to start with the simplest model that could
address your requirements, build an end-to-end prototype including this
model, and judge its performance not simply in terms of optimization metrics
but in terms of your product goal.

\subsection*{Start with a Simple Pipeline}
To do this, we will need to build a pipeline that can take data in and
return results. For most ML problems, there are actually two separate
pipelines to consider:
\begin{enumerate}
    \item[i. Training] 
    
    A training pipeline ingests all of the labeled data you would like
    to train on and passes it to a model. It then trains said model on
    the dataset until it reaches satisfactory performance.
    
    Most often, a training pipeline is used to train multiple models
    and compare their performance on a held-out validation set.


    \item[ii. Inference/Prediction] 

    At a high level, an inference pipeline starts by accepting input
    data and preprocessing it.
    
    The preprocessing phase usually consists of multiple steps. Most
    commonly, these steps will include cleaning and validating the
    input, generating features a model needs, and formatting the data
    to a numerical representation appropriate for an ML model.
    
    Pipelines in more complex systems also often need to fetch
    additional information the model needs such as user features
    stored in a database.


    \item[iii. Evaluation]
    
    From the predictions, what can we improve?
\end{enumerate}




% ML Eng, 2020. Burkov
\subsection*{Properties of a Successful Model}
A successful model has the following four properties:
\begin{itemize}
    \item it respects the input and output specifications and the performance requirement,
    \item it benefits the organization (measured via cost reduction, increased sales or profit),
    \item it helps the user (measured via productivity, engagement, and sentiment),
    \item it is scientifically rigorous.
\end{itemize}





% ML Eng, 2020. Burkov
\subsection{Structuring a Machine Learning Team}




% ML Eng, 2020. Burkov
\subsection{Why Machine Learning Projects Fail}
According to various estimates made between 2017 and 2020,
from 74\% to 87\% of machine learning and advanced analytics
projects fail or don't reach production. The reasons for
a failure range from organizational to engineering.
In this section, we consider the most impactful of them.



\subsection*{Lack of Experienced Talent}
Both data science and machine learning engineering are
relatively new disciplines. There's still no standard
way to teach them. Most organizations don't know how
to hire experts in machine learning and how to compare
them.

A significant fraction of the workforce has superficial
expertise in machine learning obtained on toy datasets
in a classroom context. Many don't have experience with
the entire machine learning project life cycle.



\subsection*{Lack of Support by the Leadership}
Scientists and software engineers often have different goals,
motivations, and success criteria
\footnote{
    In a typical Agile organization, software engineering
    teams work in sprints with clearly defined expected
    deliverables and little uncertainty.

    Scientists, on the other hand, work in high uncertainty
    and move ahead with multiple experiments. Most of such
    experiments don't result in any deliverable and, thus,
    can be seen by inexperienced leaders as no progress.
    Sometimes, after the model is built and deployed,
    the entire process has to start over because the model
    doesn't result in the expected increase of the metric
    the business cares about. Again, this can lead to the
    perception of the scientist's work by the leadership
    as wasted time and resources.
}

Furthermore, in many organizations, leaders responsible for
data science and artificial intelligence (AI), especially at
the vice-president level, have a non-scientific or even
nonengineering background. They don't know how AI works,
or have a very superficial or overly optimistic understanding
of it drawn from popular sources. They might have such a mindset
that with enough resources, technical and human, AI can solve
any problem in a short amount of time. When fast progress doesn't
happen, they easily blame scientists or entirely lose interest
in AI as an ineffective tool with hard-to-predict and uncertain
results.

Often, the problem lies in the inability of scientists to
communicate the results and challenges to upper management.
Because they don't share the vocabulary and have very different
levels of technical expertise, even a success presented badly
can be seen as a failure.

This is why, in successful organizations, data scientists are
good popularizers, while top-level  managers, responsible for
AI and analytics, often have a technical or scientific background.



\subsection*{Missing Data Infrastructure}
Scientists usually obtain the data for training by using various
ad-hoc scripts; they also use different scripts and tools to combine
various data sources. Once the model is ready, it turns out that it's
impossible, by using the available production infrastructure,
to generate input examples for the model fast enough.



\subsection*{Data Labeling Challenge}
In most machine learning projects, analysts use labeled data.
This data is usually custom, so labeling is executed specifically
for each project. It is frequent that AI and data science teams
label training data on their own and this results in a significant
time spent by skilled data scientists on data labeling and labeling
tool development. This is a major challenge for the effective execution
of an AI project.



\subsection*{Siloed Organizations and Lack of Collaboration}
Data needed for a machine learning project often resides within an organization in different
places with different ownership, security constraints, and in different formats. In siloed
organizations, people responsible for different data assets might not know one another.
Lack of trust and collaboration results in friction when one department needs access to the data
stored in a different department. Furthermore, different branches of one organization often
have their own budgets, so collaboration becomes complicated because no side has an interest
in spending their budget helping to the other side.



\subsection*{Technically Infeasible Projects}
Because of the high cost of many machine learning projects (due to high expertise and infrastructure
cost) some organizations, to “recoup the investment,” might target very ambitious
goals: to completely transform the organization or the product or provide unrealistic return or
investment. This results in very large-scale projects, involving collaboration between multiple
teams, departments, and third-parties, and pushing those teams to their limits.

As a result, such overly ambitious projects could take months or even years to complete;
some key players, including leaders and key scientists, might lose interest in the project or
even leave the organization. The project could eventually be deprioritized, or, even when
completed, be too late to the market. It is best, at least in the beginning, to focus on achievable
projects, involving simple collaboration between teams, easy to scope, and targeting a simple
business objective.





\subsection*{Lack of Alignment Between Technical and Business Teams}
Many machine learning projects start without a clear understanding, by the technical team, of
the business objective. Scientists usually frame the problem as classification or regression with
a technical objective, such as high accuracy or low mean squared error. Without continuous
feedback from the business team on the achievement of a business objective (such as an
increased click-through rate or user retention), the scientists often reach an initial level of
model performance (according to the technical objective), and then they are not sure if they
are making any useful progress and whether an additional effort is worth it. In such situations,
the projects end up being put on the shelf because time and resources were spent but the
business team didn't accept the result.


