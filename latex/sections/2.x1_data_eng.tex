% Huyen, 2022. DMLS
\section{Data Engineering}
Large data systems, even without ML, are complex. It's easy to get
lost in acronyms.  Industry standards, if there are any, evolve
quickly, creating a dynamic and ever-changing environment. If you
look into the data stack for different tech companies, it might seem
like each is doing its own thing.

Storing data is only interesting if you intend on retrieving that
data later which requires to know not only how it's formatted but
also how it's structured. Data models define how the data stored in
a particular data format is structured.

Knowing how to collect, process, store, retrieve, and process an
increasingly growing amount of data is essential to people who want
to build ML systems in production
\footnote{
    Honorable mention of data engineering from a systems perspective:
    Martin Kleppmann's excellent book Designing Data-Intensive
    Applications (O'Reilly, 2017).
}.

Overview of this section:
\begin{itemize}
    \item Data Sources.
    \item Formats for Data Storage.
    \item Data Retrieval.
    \item Data Types.
\end{itemize}



% Huyen, 2022. DMLS
\subsection{Data Sources}
ML Systems can use data from many different sources and with many
characteristics for many purposes and requirements
\footnote{
    Understanding the sources your data comes from may help you use
    your data more efficiently.
}.

Different sources of data:
\begin{itemize}
    \item \textbf{User Input data} is data explicitly input by users
    \footnote{
        If it's even remotely possible for users to input wrong data,
        they are going to do it. User input data requires more
        heavy-duty checking and processing.
    
        On top of that, users also have little patience. In most
        cases, when we input data, we expect to get results back
        immediately. Therefore, user input data tends to require
        fast processing.
    }
    (e.g. text for queries, images, videos, etc.).
    
    \item \textbf{System Generated data}: is the data generated by
    different components of your systems (e.g. various types of logs
    and system outputs such as model predictions).

    Logs provide visibility into whan and how the is the system doing.
    This helps with debugging, error detection. Log most important
    things but figure out a way to store rapidly growing logs
    \footnote{
        In most cases, you only have to store logs for as long as
        they are useful and can discard them when they are no longer
        relevant for you to debug your current system. If you don't
        have to access your logs frequently, they can also be stored
        in low-access storage that costs much less than
        higher-frequency-access storage.
    }.

    \item \textbf{Internal Databases} generated by various services
    and apps (e.g. inventories, customer transactions, user's data,
    etc.). 
    
    \item \textbf{Third Party Data} is the data collected on the
    public who are not their direct customers
    \footnote{
        \textbf{second party data} is the data collected by another
        company on their own customers that they make available
        based on some agreement.
    }.
\end{itemize}
Issues to consider: regulatory or internal requirements, and
consideration to the user's privacy.



% Huyen, 2022. DMLS
\subsection{Data Formats}
It is important to think about how the data will be used in the 
future so that the format you use will make sense. Here are some 
points to consider:
\begin{itemize}
    \item Where do I store my data so that it's cheap and still
    fast to access?

    \item How do I store complex models so that they can be loaded
    and run correctly on different hardware?

    \item How do I store multimodal data, e.g., a sample that might
    contain both images and texts?
\end{itemize}

\textbf{Data serialization} is the process of converting a data
structure or object state into a format that can be stored or
transmitted and reconstructed later.

Modern computers are very efficient at processing sequential data
compared to non-sequential data. An important distinction of data
formats are
\footnote{
    One subtle point that a lot of people don't pay attention to,
    which leads to misuses of pandas, is that this library is
    built around the columnar format. As a contrast, in NumPy
    the major order can be specified.
}:
\begin{itemize}
    \item \textbf{Row-Major} which represents the csv paradigm
    and this means that consecutive elements in a row are stored
    next to each other in memory.

    The main advantage of row-major data formats is that it is
    faster to access entire rows, like consulting all the values
    of a particular example. Another advantage is writing data.

    \item \textbf{Column-Major} representing the parquet paradigm
    this means that consecutive elements in a column are stored
    next to each other in memory.
    
    The main advantage of row-major data formats is that it is
    faster to access entire attributes or features, like
    consulting all the timestamps as a particular feature.
    The other main advantage is reading data.
\end{itemize}

For a comprehensive list of the data formats consult:
%\url{
%    https://en.wikipedia.org/wiki/Comparison_of_data-serialization_formats
%}



% Huyen, 2022. DMLS
\subsection{Data Models}
Data models describe how data is represented. How you choose to
represent data not only affects the way your systems are built,
but also the problems your systems can solve.

There aretwo types of models that seem opposite to each other
but are actually converging: relational models and NoSQL models.


\subsection*{Relational Model}
Relational models are among the most persistent ideas in computer
science. Invented by Edgar F. Codd in 1970, the relational model
is still going strong today. The idea is simple but powerful.
In this model, data is organized into relations
\footnote{
    A table is an accepted visual representation of a relation, 
    and each row of a table makes up a tuple.

};
each relation is a set of tuples. Data following the relational
model is usually stored in file formats like CSV or Parquet.

Databases built around the relational data model are relational
databases. Once you've put data in your databases, you'll want
a way to retrieve it. The language that you can use to specify
the data that you want from a database is called a query
language. The most popular query language for relational
databases today is SQL.



\subsection*{NoSQL}
For certain use cases, the relational data can be restrictive
\footnote{
    e.g. schema management is painful; complex queries for
    specialized applications.
}.
The latest movement against the relational data model is NoSQL.
NoSQL has been retroactively reinterpreted as Not Only SQL,
as many NoSQL data systems also support relational models.

Two major types of nonrelational models are:
\begin{itemize}
    \item the document model that targets use cases where
    data comes in self-contained documents and relationship
    between one document and another are rare.

    \item the graph model goes in the opposite direction,
    targeting use cases where relationships between data
    items are common and important.
\end{itemize}




% insert section 2.1.2
%\subsection{Data Storage Engines and Processing}





















% Huyen, 2022. DMLS
\subsection{Modes of Dataflow}
Most of the time, in production, you don't have a single process
but multiple. A question arises: how do we pass data between
different processes that don't share memory?

When data is passed from one process to another, we say that the
data flows from one process to another, which gives us a dataflow.
There are three main modes of dataflow:
\begin{itemize}
    \item \textbf{Data passing through databases.}
    
    \noindent
    The easiest way to pass data between two processes is through
    databases
    \footnote{
        e.g. to pass data from process A to process B, process A
        can write that data into a database, and process B simply
        reads from that database.
    }.
    This mode, however, doesn't always work because of two reasons.
    \begin{enumerate}
        \item it requires that both processes must be able to
        access the same database. This might be infeasible.

        \item it requires both processes to access data from
        databases, and read/write from databases can be slow,
        making it unsuitable for applications with strict latency
        requirements
        \footnote{
            e.g., almost all consumer-facing applications.
        }.
    \end{enumerate}

    
    \item \textbf{Data passing through services.}
    
    \noindent
    One way to pass data between two processes is to send data
    directly through a network that connects these two processes
    using requests such as the requests provided by REST and RPC
    APIs (e.g., POST/GET requests).

    To pass data from process B to process A, process A first sends
    a request to process B that specifies the data A needs, and B
    returns the requested data through the same network. Because
    processes communicate through requests, we say that this is
    request-driven.
    
    This mode of data passing is tightly coupled with the
    service-oriented architecture. A service is a process that
    can be accessed remotely
    \footnote{
        Two services in communication with each other can be run
        by different companies in different applications. Two
        services in communication with each other can also be
        parts of the same application.

        Structuring different components of your application as
        separate services allows each component to be developed,
        tested, and maintained independently of one another.
        
        Structuring an application as separate services gives you
        a microservice architecture.
    }, e.g., through a network.
    
    The most popular styles of requests used for passing data
    through networks are REST (representational state transfer)
    and RPC (remote procedure call).
    
    \item \textbf{Data passing through a real-time transport.}

    \noindent
    like Apache Kafka and Amazon Kinesis.
\end{itemize}

















% sumarize next text
\subsection{Batch Processing Versus Stream Processing}
Data once stored becomes historical data, as oposed to streaming 
data.

Historical data is usually processed in batches due to
convenience
\footnote{
    spark exists to procces batch data efficiently.
}. Batch processing is mainly used for features that change less
frequently (e.g. static features a.k.a batch features).

When the data is in real time transport
\footnote{
    for example, in Apache Kafka or Amazon Kinesis
},
it is streaming data. \textbf{Stream processing} refers to doing
computations on streaming data. The main benefit of stream
processing is the leverage of dynamic or streaming features that
change in a short span of time and are relevant to the system.

Stream processing can be done periodically but the time interval is
usually much shorter, but when done right it can give low latency
because you can process data as soon as data is generated, without
having to first write it into databases.

Some applications require static, dynamic, or both kinds of features.
Make sure the data infraestructure supports all the requirements of
the system. Streaming computations are rarely simple.











%  https://bpostance.github.io/posts/introduction-to-hashing/
%  https://mauricebrg.com/2022/09/efficiently-hashing-columns-in-a-pandas-dataframe.html
%  https://mauricebrg.com/2022/12/even-more-efficient-hashing-of-columns-in-a-pandas-dataframe.html
%  https://medium.com/@serene_mulberry_tiger_125/the-magic-of-hashing-for-efficient-data-retrieval-37bed5c927a5
\subsection{Indentification of Duplicated Data: Hashing}
We assume a given dataframe, that is, a given data structure with
2 dimensions (columns and rows). Messy data is abundant in business
\footnote{
    Most people working with data are familiar with this problem and
    there is an arsenal of tools at our disposal. Including:
    text manipulation and standardisation e.g. cleanco,
    regex; clustering; and ML techniques entity 
    resolution, record linkage and deduplication.
}.
A simple and powerful solution needs to:
\begin{enumerate}
    \item create a unique identifier for each record.
    \item efficiently indicate whether any new records were 
    duplicates of existing ones.
\end{enumerate}
Cue a hash function
\footnote{
    A hash function is a deterministic function that maps inputs of
    arbitrary sizes to outputs of a fixed size.

    Some important properties of Hash functions are:
    \begin{itemize}
        \item Quick to compute.
        \item Deterministic, i.e. they are non-random and repeatable.
        \item Even if the inputs are similar such as “ABC” and “ACB”,
        the outputs should be uncorrelated.
        \item They can map an infinite number of inputs, and of any
        length, to outputs of fixed length.
    \end{itemize}

    These properties make hashes a good candidate for creating a
    unique identifier for a row in a dataframe. Common hash function
    applications include cryptography and creating hash-tables for
    indexing and searching data.
}.
To create a hash over multiple columns, we concatenate the values in
these columns, feed them into a hash function, and store the output.
But, before hashing we must consider cleaning the data because
hashing is highly sensitive to even minor discrepancies in the data.
Any discrepancies, such as extra whitespace, different casing, or
missing values, can result in different hash values for rows that are
logically the same. Below are some common reasons why data cleaning is
necessary before hashing:
\begin{enumerate}
    \item \textbf{Standardizing Column Order}

    \noindent
    If the column order differs across DataFrames, the same data can
    produce different hashes.  Ensure the column order is consistent 
    by sorting the columns.


    \item \textbf{Ensuring Consistent Data Types}

    \noindent
    Mixed types in a column (e.g., int vs. str for the same data)
    will produce different hashes. Make sure to convert all data to
    consistent types (e.g. all str).


    \item \textbf{Dealing with Missing Values.}

    \noindent
    Missing or NaN values in the data can cause inconsistent hashes.
    This is easily solved with a standard representation 
    (e.g. “:” as a separator in place of missing or NaN values).


    \item \textbf{Handling Inconsistent Fomatting.}
    
    \noindent
    Strings with different cases ("Alice" vs. "alice") or extra spaces
    ("Alice " vs. "Alice") will produce different hashes. The solution
    is to standardize the format of string columns
    (e.g., trim whitespace, convert to lowercase).


    \item \textbf{Eliminating Special Characters}

    \noindent
    Unwanted special characters (e.g.,
    \textbackslash n, \textbackslash t, or invisible control characters)
    can lead to different string representations. So, clean strings to
    remove special characters.
\end{enumerate}











It's important to add a separator between the values. Otherwise, you
may run into problems when one of the values is empty.


